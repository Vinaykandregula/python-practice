{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336c00ea",
   "metadata": {},
   "source": [
    "## Web Scraping \n",
    "\n",
    "Web scraping is the process of extracting specific data on a targeted webpages.\n",
    "\n",
    " or\n",
    " \n",
    " \n",
    "we write some code to fetch data from websites in an automated fashion..\n",
    "\n",
    "#### web crawling: \n",
    "\n",
    "is like search engine,it goes through different webpages without specific goal.\n",
    "\n",
    "Websites with dynamic content(changes with user need) cannot be scraped using BeautifulSoup. One way to scrape dynamic website is by using Selenium.\n",
    "\n",
    "#### 1. Pick a website and describe your objective\n",
    "\n",
    "- Browse through different sites and pick on to scrape. Check the \"Project Ideas\" section for inspiration.\n",
    "- Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.\n",
    "- Summarize your project idea and outline your strategy in a Juptyer notebook. Use the \"New\" button above.\n",
    "\n",
    "#### 2. Use the requests library to download web pages\n",
    "- Inspect the website's HTML source and identify the right URLs to download.\n",
    "- Download and save web pages locally using the requests library.\n",
    "- Create a function to automate downloading for different topics/search queries.\n",
    "\n",
    "#### 3.Use Beautiful Soup to parse and extract information\n",
    "- Parse and explore the structure of downloaded web pages using Beautiful soup.\n",
    "- Use the right properties and methods to extract the required information.\n",
    "- Create functions to extract from the page into lists and dictionaries.\n",
    "- (Optional) Use a REST API to acquire additional information if required.\n",
    "#### 4.Create CSV file(s) with the extracted information\n",
    "- Create functions for the end-to-end process of downloading, parsing, and saving CSVs.\n",
    "- Execute the function with different inputs to create a dataset of CSV files.\n",
    "- Verify the information in the CSV files by reading them back using Pandas.\n",
    "\n",
    "#### 5.Document and share your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d571f",
   "metadata": {},
   "source": [
    "### Scraping-Github-topics-repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade46ab2",
   "metadata": {},
   "source": [
    "### Problem Statement:\n",
    "\n",
    "Find top 20 repositories in each github topics of 3D,ajax etc..\n",
    "\n",
    "Before jumping directy for coding its better to prepare output by hands using sheets.new. then we can get basic idea of how final outcome looks as like.\n",
    "\n",
    "#### 1. Pick a website and describe your objective\n",
    "\n",
    "- Browse through different sites and pick on to scrape. Check the \"Project Ideas\" section for inspiration.\n",
    "- Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.\n",
    "- Summarize your project idea and outline your strategy in a Juptyer notebook. Use the \"New\" button above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4602aa0",
   "metadata": {},
   "source": [
    "#### Project Outline:\n",
    "- we're going to scrape github topics https://github.com/topics\n",
    "- we'll get a list of topics. For each topic, we'll get topic title, topic url & topic description\n",
    "- for each topic,pick top 25 repositories in the topic from the topic page\n",
    "- for each repository we grab repo name,username,star,url.\n",
    "- for each topic we'll create a csv file in the following format:\n",
    "\n",
    "```\n",
    "Repo Name,username,stars,repo url\n",
    "three.js,mrdood,93.4,https://github.com/mrdoob/three.js\n",
    "react-three-fiber,pmndrs,23.3,https://github.com/pmndrs/react-three-fiber\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4a9ed",
   "metadata": {},
   "source": [
    "#### 2. Use the requests library to download web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0774f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b024a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ddcd2",
   "metadata": {},
   "source": [
    "#### 3.Use Beautiful Soup to parse and extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7471cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.9/site-packages (23.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e8d113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc2641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9a2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls():\n",
    "    url='https://github.com/topics'\n",
    "    response=requests.get(url)\n",
    "    content=response.text\n",
    "    doc=BeautifulSoup(content,'lxml')\n",
    "    p_tag=doc.find_all('p',class_='f3 lh-condensed mb-0 mt-1 Link--primary')\n",
    "    des_tag=doc.find_all('p',class_='f5 color-fg-muted mb-0 mt-1')\n",
    "    url_tag=doc.find_all('a','no-underline flex-1 d-flex flex-column')  \n",
    "    def topic_info(p_tag,url_tag,des_tag):\n",
    "        repo_url='https://github.com'+url_tag['href']\n",
    "        des_info=des_tag.text.strip()\n",
    "        for i in p_tag:\n",
    "            name=i.text.strip()\n",
    "        return name,repo_url,des_info\n",
    "    topics={'Topic name':[],'Description':[],'url':[]}\n",
    "    for i in range(len(url_tag)):\n",
    "        topic_link=topic_info(p_tag[i],url_tag[i],des_tag[i])\n",
    "        topics['Topic name'].append(topic_link[0])\n",
    "        topics['Description'].append(topic_link[2])\n",
    "        topics['url'].append(topic_link[1])\n",
    "    return pd.DataFrame(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab6c224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic name</th>\n",
       "      <th>Description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D</td>\n",
       "      <td>3D refers to the use of three-dimensional grap...</td>\n",
       "      <td>https://github.com/topics/3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajax</td>\n",
       "      <td>Ajax is a technique for creating interactive w...</td>\n",
       "      <td>https://github.com/topics/ajax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithm</td>\n",
       "      <td>Algorithms are self-contained sequences that c...</td>\n",
       "      <td>https://github.com/topics/algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amp</td>\n",
       "      <td>Amp is a non-blocking concurrency library for ...</td>\n",
       "      <td>https://github.com/topics/amphp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Android</td>\n",
       "      <td>Android is an operating system built by Google...</td>\n",
       "      <td>https://github.com/topics/android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic name                                        Description  \\\n",
       "0         3D  3D refers to the use of three-dimensional grap...   \n",
       "1       Ajax  Ajax is a technique for creating interactive w...   \n",
       "2  Algorithm  Algorithms are self-contained sequences that c...   \n",
       "3        Amp  Amp is a non-blocking concurrency library for ...   \n",
       "4    Android  Android is an operating system built by Google...   \n",
       "\n",
       "                                   url  \n",
       "0         https://github.com/topics/3d  \n",
       "1       https://github.com/topics/ajax  \n",
       "2  https://github.com/topics/algorithm  \n",
       "3      https://github.com/topics/amphp  \n",
       "4    https://github.com/topics/android  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4756fe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D https://github.com/topics/3d\n",
      "Ajax https://github.com/topics/ajax\n",
      "Algorithm https://github.com/topics/algorithm\n",
      "Amp https://github.com/topics/amphp\n",
      "Android https://github.com/topics/android\n",
      "Angular https://github.com/topics/angular\n",
      "Ansible https://github.com/topics/ansible\n",
      "API https://github.com/topics/api\n",
      "Arduino https://github.com/topics/arduino\n",
      "ASP.NET https://github.com/topics/aspnet\n",
      "Atom https://github.com/topics/atom\n",
      "Awesome Lists https://github.com/topics/awesome\n",
      "Amazon Web Services https://github.com/topics/aws\n",
      "Azure https://github.com/topics/azure\n",
      "Babel https://github.com/topics/babel\n",
      "Bash https://github.com/topics/bash\n",
      "Bitcoin https://github.com/topics/bitcoin\n",
      "Bootstrap https://github.com/topics/bootstrap\n",
      "Bot https://github.com/topics/bot\n",
      "C https://github.com/topics/c\n",
      "Chrome https://github.com/topics/chrome\n",
      "Chrome extension https://github.com/topics/chrome-extension\n",
      "Command line interface https://github.com/topics/cli\n",
      "Clojure https://github.com/topics/clojure\n",
      "Code quality https://github.com/topics/code-quality\n",
      "Code review https://github.com/topics/code-review\n",
      "Compiler https://github.com/topics/compiler\n",
      "Continuous integration https://github.com/topics/continuous-integration\n",
      "COVID-19 https://github.com/topics/covid-19\n",
      "C++ https://github.com/topics/cpp\n"
     ]
    }
   ],
   "source": [
    "for index, row in urls().iterrows():\n",
    "    print(row[\"Topic name\"], row[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b471b",
   "metadata": {},
   "source": [
    "#### Getting information out of a topic_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2823966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def sub_topic_link(topic_url):\n",
    "    response=requests.get(topic_url)\n",
    "    if response.status_code!=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    content=BeautifulSoup(response.text,'lxml')\n",
    "    h_tag=content.find_all('h3','f3 color-fg-muted text-normal lh-condensed')\n",
    "    star_tag=content.find_all('span',class_='Counter js-social-count')\n",
    "    def star(star_tag):\n",
    "        star_tag.strip()\n",
    "        if star_tag[-1]=='k':\n",
    "            return int(float(star_tag[:-1])*1000)\n",
    "        return int(star_tag)\n",
    "    def sub_topic_details(h_tag,star_tag):\n",
    "        a_tag=h_tag.find_all('a',class_='Link')\n",
    "        username=a_tag[0].text.strip()\n",
    "        repo_name=a_tag[1].text.strip()\n",
    "        repo_url='https://github.com'+a_tag[1]['href']\n",
    "        star_count=star(star_tag.text.strip())\n",
    "        return username,repo_name,star_count,repo_url\n",
    "    topic={'user':[],'repository':[],'rating':[],'url':[]}\n",
    "    for i in range(len(h_tag)):\n",
    "        topic_details=sub_topic_details(h_tag[i],star_tag[i])\n",
    "        topic['user'].append(topic_details[0])\n",
    "        topic['repository'].append(topic_details[1])\n",
    "        topic['rating'].append(topic_details[2])\n",
    "        topic['url'].append(topic_details[3])\n",
    "    return pd.DataFrame(topic)\n",
    "\n",
    "#help(os.path)\n",
    "\n",
    "#help(os.path.exists)\n",
    "\n",
    "def save_topics(topic_url,topic_name): #topic_name--path\n",
    "    if os.path.exists(topic_name):\n",
    "        print('The file {} already exists. Skip..'.format(topic_name))\n",
    "        return\n",
    "    df=sub_topic_link(topic_url)\n",
    "    df.to_csv(topic_name,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe51df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_infos():\n",
    "    print('scraping Topics')\n",
    "    topic_df=urls()\n",
    "    \n",
    "    os.makedirs('Scraping Data',exist_ok=True)\n",
    "    \n",
    "    for index,row in topic_df.iterrows():\n",
    "        print('Scraping Top repositories of \"{}\"'.format(row['Topic name']))\n",
    "        save_topics(row['url'],'Scraping Data/{}.csv'.format(row['Topic name']))\n",
    "    print('Scraping Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f1b0d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping Topics\n",
      "Scraping Top repositories of \"3D\"\n",
      "Scraping Top repositories of \"Ajax\"\n",
      "Scraping Top repositories of \"Algorithm\"\n",
      "Scraping Top repositories of \"Amp\"\n",
      "Scraping Top repositories of \"Android\"\n",
      "Scraping Top repositories of \"Angular\"\n",
      "Scraping Top repositories of \"Ansible\"\n",
      "Scraping Top repositories of \"API\"\n",
      "Scraping Top repositories of \"Arduino\"\n",
      "Scraping Top repositories of \"ASP.NET\"\n",
      "Scraping Top repositories of \"Atom\"\n",
      "Scraping Top repositories of \"Awesome Lists\"\n",
      "Scraping Top repositories of \"Amazon Web Services\"\n",
      "Scraping Top repositories of \"Azure\"\n",
      "Scraping Top repositories of \"Babel\"\n",
      "Scraping Top repositories of \"Bash\"\n",
      "Scraping Top repositories of \"Bitcoin\"\n",
      "Scraping Top repositories of \"Bootstrap\"\n",
      "Scraping Top repositories of \"Bot\"\n",
      "Scraping Top repositories of \"C\"\n",
      "Scraping Top repositories of \"Chrome\"\n",
      "Scraping Top repositories of \"Chrome extension\"\n",
      "Scraping Top repositories of \"Command line interface\"\n",
      "Scraping Top repositories of \"Clojure\"\n",
      "Scraping Top repositories of \"Code quality\"\n",
      "Scraping Top repositories of \"Code review\"\n",
      "Scraping Top repositories of \"Compiler\"\n",
      "Scraping Top repositories of \"Continuous integration\"\n",
      "Scraping Top repositories of \"COVID-19\"\n",
      "Scraping Top repositories of \"C++\"\n",
      "Scraping Done.\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_infos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0b077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09bac33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e322b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
